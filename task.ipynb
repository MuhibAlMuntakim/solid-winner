{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (4.29.0)\n",
      "Requirement already satisfied: webdriver-manager in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from selenium) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from webdriver-manager) (1.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from webdriver-manager) (24.2)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (3.10)\n",
      "Requirement already satisfied: outcome in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from requests->webdriver-manager) (3.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.22)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\user\\.conda\\envs\\solid\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install required libraries\n",
    "%pip install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Full scraping script with deduplication\n",
    "\n",
    "import time\n",
    "import csv\n",
    "import urllib.parse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "def clean_website(display_text, href):\n",
    "    \"\"\"\n",
    "    Refine the website URL by using the display text if available \n",
    "    and removing UTM parameters.\n",
    "    \"\"\"\n",
    "    text = display_text.strip() if display_text else \"\"\n",
    "    website = text if text else href.strip()\n",
    "    # Remove query parameters (like UTM)\n",
    "    parsed = urllib.parse.urlparse(website)\n",
    "    if parsed.scheme and parsed.netloc:\n",
    "        website = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "    return website if website else \"N/A\"\n",
    "\n",
    "def get_contact_details(driver, contact_card, max_attempts=2):\n",
    "    \"\"\"\n",
    "    Try clicking the \"Contact\" button and extracting details.\n",
    "    Returns (website, email, location). Retries if necessary.\n",
    "    \"\"\"\n",
    "    website = \"N/A\"\n",
    "    email = \"N/A\"\n",
    "    location = \"N/A\"\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            contact_button = contact_card.find_element(By.XPATH, './/button[@aria-label=\"Contact\"]')\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", contact_button)\n",
    "            driver.execute_script(\"arguments[0].click();\", contact_button)\n",
    "            # Increase sleep time on retry\n",
    "            time.sleep(2 if attempt else 1)\n",
    "            \n",
    "            wait = WebDriverWait(driver, 10)\n",
    "            tooltip = wait.until(EC.visibility_of_element_located(\n",
    "                (By.CSS_SELECTOR, 'div.tooltip_tooltip-inner___wDGV')\n",
    "            ))\n",
    "            \n",
    "            li_elements = tooltip.find_elements(By.XPATH, './/li[contains(@class, \"styles_item__9pNrw\")]')\n",
    "            # Reset values for this attempt\n",
    "            website, email, location = \"N/A\", \"N/A\", \"N/A\"\n",
    "            for li in li_elements:\n",
    "                try:\n",
    "                    a_tag = li.find_element(By.TAG_NAME, \"a\")\n",
    "                    href = a_tag.get_attribute(\"href\")\n",
    "                    if href.startswith(\"mailto:\"):\n",
    "                        email = href.replace(\"mailto:\", \"\").strip() or \"N/A\"\n",
    "                    elif href.startswith(\"tel:\"):\n",
    "                        # Skip phone numbers.\n",
    "                        continue\n",
    "                    elif href.startswith(\"http\"):\n",
    "                        website = clean_website(a_tag.text, href)\n",
    "                except Exception:\n",
    "                    text_val = li.text.strip()\n",
    "                    if text_val:\n",
    "                        location = text_val\n",
    "            if website != \"N/A\" or email != \"N/A\" or location != \"N/A\":\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed to extract contact details: {e}\")\n",
    "            time.sleep(2)\n",
    "    return website, email, location\n",
    "\n",
    "def scrape_current_page(driver):\n",
    "    \"\"\"\n",
    "    Scrape all listings from the current page.\n",
    "    Returns a list of dictionaries with keys: Business Name, Website, Location, Email.\n",
    "    \"\"\"\n",
    "    page_data = []\n",
    "    \n",
    "    # Collect business name cards and contact cards separately.\n",
    "    business_cards = driver.find_elements(By.CSS_SELECTOR, \n",
    "        \"div.paper_paper__EGeEb.paper_outline__bqVmn.card_card__yyGgu.card_noPadding__OOiac.styles_wrapper__Jg8fe\")\n",
    "    contact_cards = driver.find_elements(By.CSS_SELECTOR, \n",
    "        \"div.card_cardContent__4Js_A.styles_footerWrapper__fzSEA\")\n",
    "    \n",
    "    total = min(len(business_cards), len(contact_cards))\n",
    "    for i in range(total):\n",
    "        # Extract business name.\n",
    "        try:\n",
    "            bn_elem = business_cards[i].find_element(By.CSS_SELECTOR, \n",
    "                        \"p.typography_heading-xs__osRhC.typography_appearance-default__t8iAq\")\n",
    "            business_name = bn_elem.text.strip() or \"N/A\"\n",
    "            if i == 0 and not business_name:\n",
    "                time.sleep(2)\n",
    "                business_name = bn_elem.text.strip() or \"N/A\"\n",
    "        except Exception:\n",
    "            business_name = \"N/A\"\n",
    "        \n",
    "        # Extract contact details from the corresponding contact card.\n",
    "        website, email, location = get_contact_details(driver, contact_cards[i])\n",
    "        \n",
    "        page_data.append({\n",
    "            \"Business Name\": business_name,\n",
    "            \"Website\": website,\n",
    "            \"Location\": location,\n",
    "            \"Email\": email\n",
    "        })\n",
    "    return page_data\n",
    "\n",
    "def scrape_category(driver, category, max_items=40):\n",
    "    \"\"\"\n",
    "    Scrape listings from a category across multiple pages until max_items are collected.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    base_url = f\"https://www.trustpilot.com/categories/{category}\"\n",
    "    driver.get(base_url)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    while len(data) < max_items:\n",
    "        current_page_data = scrape_current_page(driver)\n",
    "        data.extend([{\"Category\": category, **d} for d in current_page_data])\n",
    "        print(f\"Collected {len(data)} listings so far for {category}...\")\n",
    "        if len(data) >= max_items:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            next_page_button = driver.find_element(By.CSS_SELECTOR, 'a[name=\"pagination-button-next\"]')\n",
    "            next_href = next_page_button.get_attribute(\"href\")\n",
    "            if next_href:\n",
    "                next_url = urllib.parse.urljoin(\"https://www.trustpilot.com\", next_href)\n",
    "                driver.get(next_url)\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"No next page found or error navigating: {e}\")\n",
    "            break\n",
    "    \n",
    "    return data[:max_items]\n",
    "\n",
    "def deduplicate_records(records):\n",
    "    \"\"\"\n",
    "    Deduplicate records based on a unique key created from all fields.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    deduped = []\n",
    "    for rec in records:\n",
    "        key = (\n",
    "            rec[\"Category\"].strip().lower(),\n",
    "            rec[\"Business Name\"].strip().lower(),\n",
    "            rec[\"Website\"].strip().lower(),\n",
    "            rec[\"Email\"].strip().lower(),\n",
    "            rec[\"Location\"].strip().lower()\n",
    "        )\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(rec)\n",
    "    return deduped\n",
    "\n",
    "def main():\n",
    "    categories = [\"beauty_wellbeing\", \"food_beverages_tobacco\", \"electronics_technology\"]\n",
    "    all_data = []\n",
    "    \n",
    "    options = Options()\n",
    "    options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    \n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "    \n",
    "    # Loop through categories and scrape up to max_items for each.\n",
    "    for cat in categories:\n",
    "        print(f\"Scraping category: {cat}\")\n",
    "        cat_data = scrape_category(driver, cat, max_items=100)\n",
    "        all_data.extend(cat_data)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    # Deduplicate records.\n",
    "    all_data = deduplicate_records(all_data)\n",
    "    \n",
    "    # Write the collected data to CSV.\n",
    "    csv_file = \"trustpilot_data.csv\"\n",
    "    fieldnames = [\"Category\", \"Business Name\", \"Website\", \"Location\", \"Email\"]\n",
    "    try:\n",
    "        with open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "            for row in all_data:\n",
    "                writer.writerow(row)\n",
    "        print(f\"Scraping complete. {len(all_data)} deduplicated records saved to {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error writing CSV:\", e)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "solid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
